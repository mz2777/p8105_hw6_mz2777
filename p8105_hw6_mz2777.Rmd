---
title: "Homework 6"
author: "Mengyu Zhang"
date: "11/16/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(MASS)
library(modelr)
```

# Problem 1

### Clean Data

```{r}
# import data
birthweight = read_csv("data/birthweight.csv") %>% 
  janitor::clean_names()

# taking a quick look
glimpse(birthweight)

birthweight = birthweight %>% 
  mutate(
    babysex = factor(babysex, levels = c(1, 2), labels = c("male", "female")),
    frace = factor(frace, levels = c(1, 2, 3, 4, 8, 9), labels = c("White", "Black", "Asian", "Puerto Rican", "Other", "Unknown")),
    malform = factor(malform, levels = c(0, 1), labels = c("absent", "present")),
    mrace = factor(mrace, levels = c(1, 2, 3, 4, 8, 9), labels = c("White", "Black", "Asian", "Puerto Rican", "Other", "Unknown"))
  )
```

### check for missing data
The result shows that there is no missing data.
```{r}
sum(is.na(birthweight))
```

### Roughly Fitting
By roughly fitting a model, we can see that there are many coefficients that does not pass the t test. Therefore, we need to select some main variable to fit another model.
```{r}
rough_model = lm(bwt ~ babysex + bhead + blength + delwt + fincome + frace + gaweeks + malform + menarche + mheight + momage + mrace + parity + pnumlbw + pnumsga + ppbmi + ppwt + smoken + wtgain, data = birthweight)
summary(rough_model)
```


### Stepwise Regression

I use the stepwise regression to select predictors though this method have many limitations. Finally, we get the model with babysex, bhead, blength, delwt, fincome, gaweeks, mheight, mrace, parity, ppwt and smoken as predictors.

```{r}
# Stepwise Regression
step = stepAIC(rough_model, direction="both")
step$anova # display results
```

### Model Summary

By looking at model summary, the R-squared value of 0.72 is not bad. The F-value is highly significant implying that all the explanatory variables together significantly explain the birth weight.

However, coming to the individual regression coefficients, it is seen that two predictors ('fincome' and 'mracePuerto Rican') are significant only at 10 % level of significance.

```{r}
# my model
model = lm(bwt ~ babysex + bhead + blength + delwt + fincome + gaweeks + 
    mheight + mrace + parity + ppwt + smoken, data = birthweight)
summary(model)
```

### Diagnostics

A plot of model residuals against fitted values shows that in general, residuals and fitted values are independent which means that the predictors we select can explaint almost all informations among birth weight and its determinants.
```{r}
# Diagnostics
birthweight %>% 
  modelr::add_residuals(model) %>% 
  modelr::add_predictions(model) %>% 
  ggplot(aes(x = pred, y = resid)) +
  geom_point(alpha = .6)
```

### Comparision
From the violin plot, we can see that the model I choose generally have the smallest rmse, and the model using length at birth and gestational age as predictors have the biggest rmse.
```{r}
cv_df = 
  crossv_mc(birthweight, 100) 

cv_df =
  cv_df %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))


cv_df = 
  cv_df %>% 
  mutate(my_model = map(train, ~lm(bwt ~ babysex + bhead + blength + delwt + fincome + gaweeks + mheight + mrace + parity + ppwt + smoken, data = .x)),
         main_effect_model  = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
         interaction_model  = map(train, ~lm(bwt ~ babysex + bhead + blength + babysex * bhead + bhead * blength + babysex * blength + babysex * bhead * blength, data = .x))) %>% 
  mutate(rmse_my = map2_dbl(my_model, test, ~rmse(model = .x, data = .y)),
         rmse_main_effect = map2_dbl(main_effect_model, test, ~rmse(model = .x, data = .y)),
         rmse_interaction = map2_dbl(interaction_model, test, ~rmse(model = .x, data = .y)))
```

```{r}
cv_df %>% 
  dplyr::select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin()
```

# Problem 2

### Get and clean data
```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  dplyr::select(name, id, everything())
```
### Interval of adjusted R-square
```{r}
# Adjusted R-square
weather_df %>% 
  modelr::bootstrap(n = 5000) %>% 
  mutate(models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
         results = map(models, broom::glance)) %>% 
  dplyr::select(-strap, -models) %>% 
  unnest(results) %>% 
  dplyr::select(adj.r.squared) %>% 
  apply(2, quantile, probs = c(0.025,0.975))
```

### Interval of log(beta0*beta1)
```{r}
# log(beta0*beta1)
weather_df %>% 
  modelr::bootstrap(n = 5000) %>% 
  mutate(models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
         results = map(models, broom::tidy)) %>% 
  dplyr::select(-strap, -models) %>% 
  unnest(results) %>% 
  dplyr::select(-(std.error:p.value)) %>% 
  pivot_wider(
    names_from = term,
    values_from = estimate
  ) %>% 
  janitor::clean_names() %>% 
  mutate(log_beta = log(intercept*tmin)) %>%
  dplyr::select(log_beta) %>% 
  apply(2, quantile, probs = c(0.025,0.975))
```

